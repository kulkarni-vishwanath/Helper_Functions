{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(df):\n",
    "    # Suppressing the scientific notation\n",
    "    pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "    \n",
    "    # Converting the columns to lower case for ease of use\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Information about the shape of the dataset\n",
    "    print (\"Shape of the dataset is:{}\".format(df.shape))\n",
    "    print (\"---------------------------------------------------------\")\n",
    "    \n",
    "    # Missing columns in the dataset and the percentage of missing values\n",
    "    missing_cols = []\n",
    "    missing_pct = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            missing_cols.append(col)\n",
    "            missing_pct.append(round((df[col].isna().sum()/len(df))*100,3))\n",
    "    missing_data = pd.DataFrame({\"Pct_Missing\":missing_pct},index=missing_cols)\n",
    "    if len(missing_cols)==0:\n",
    "        print (\"There are no missing values\")\n",
    "    else:\n",
    "        print (\"Missing Data Information\")\n",
    "        print (\"---------------------------------------------------------\")\n",
    "        print (missing_data)\n",
    "    print (\"---------------------------------------------------------\")\n",
    "    \n",
    "    # Descriptive statistic of the numerical data\n",
    "    numerical_columns = list(df.select_dtypes(exclude=\"object\").columns)\n",
    "    print (\"Descriptive Statistics Of Numerical Variables\")\n",
    "    print (\"---------------------------------------------------------\")\n",
    "    print (df[numerical_columns].describe())\n",
    "    print (\"---------------------------------------------------------\")\n",
    "    \n",
    "    # Checking the number of unique values in the categorical columns in the data\n",
    "    categorical_columns = list(df.select_dtypes(include=\"object\").columns)\n",
    "    print (\"Number Unique Values in Categorical Columns\")\n",
    "    print (\"---------------------------------------------------------\")\n",
    "    for col in categorical_columns:\n",
    "        print (\"Number of Unique Values in {} Column Are:{}\".format(col,df[col].nunique()))\n",
    "    print (\"---------------------------------------------------------\")\n",
    "        \n",
    "    # Getting the range of the datetime columns, if any\n",
    "    datetype_columns = list(df.select_dtypes(include='datetime').columns)\n",
    "    if len(datetype_columns) == 0:\n",
    "        print (\"There Are No Datetime Columns In The Dataset\")\n",
    "    else :\n",
    "        print (\"Datetime Columns are:{}\".format(datetype_columns))\n",
    "        print (\"---------------------------------------------------------\")\n",
    "        for col in datetype_columns:\n",
    "            print (\"{} Datetime Column Ranges From {} to {}\".format(col,df[col].min(),df[col].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def categorical_plots(df,columns):\n",
    "    # This function basically creates the countplot of all categorical variables.\n",
    "    # columns should be a list of categorical variables.\n",
    "    for ind,col in enumerate(columns):\n",
    "        plt.figure(ind);\n",
    "        (df[col].value_counts(dropna=False)/len(df)).plot.bar();\n",
    "        plt.title(\"Distribution of {} column\".format(col));\n",
    "        plt.xlabel(col);\n",
    "        plt.ylabel(\"Percentage\");\n",
    "        plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_plots(df,columns):\n",
    "    # This small function basically creates the distribution plot of all numerical variables.\n",
    "    # columns should be a list of numerical variables.\n",
    "    for ind,col in enumerate(columns):\n",
    "        plt.figure(ind);\n",
    "        sns.distplot(df[col],kde=True);\n",
    "        plt.title(\"Distribution Plot of {} column\".format(col));\n",
    "        plt.xlabel(col);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation(df,group_by,stats,to_aggregate):\n",
    "    \"\"\"\n",
    "    This function basically creates grouped features. group_by is the list of columns to be grouped by, stats is the list of \n",
    "    statistical measure such as min, max, mean etc. to_aggregate is the list of columns whose statistical measure is to be found.\n",
    "    group_by, stats and to_aggregate should be passed as lists.\n",
    "    \"\"\"\n",
    "    for item in group_by:\n",
    "        for agg in stats:\n",
    "            for col in to_aggregate:\n",
    "                df[agg+\"_Of_\"+col+\"_GroupBy_\"+item] = df.groupby(item)[col].transform(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datetime_features(df,columns):\n",
    "    # This function basically generates different datetime features based on its attributes.\n",
    "    # columns should be a list of columns that are datetime variables. \n",
    "\n",
    "    for col in cols:\n",
    "        df[col+\"_Year\"] = df[col].dt.year\n",
    "        df[col+\"_Quarter\"] = df[col].dt.quarter\n",
    "        df[col+\"_Month\"] = df[col].dt.month\n",
    "        df[col+\"_DayOfMonth\"] = df[col].dt.day\n",
    "        df[col+\"_DayOfWeek\"] = df[col].dt.dayofweek\n",
    "        df[col+\"_Is_Month_Start\"] = df[col].dt.is_month_start\n",
    "        df[col+\"_Is_Month_End\"] = df[col].dt.is_month_end\n",
    "        df[col+\"_Is_Quarter_Start\"] = df[col].dt.is_quarter_start\n",
    "        df[col+\"_Is_Quarter_End\"] = df[col].dt.is_quarter_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation(clf,fit_params,train,test,features):\n",
    "    \n",
    "    # defining the number of splits\n",
    "    NSPLITS = 5\n",
    "    \n",
    "    # generating dummy predictions for test data and out of fold data\n",
    "    predictions  = np.zeros(test.shape[0])\n",
    "    oot_of_folds = np.zeros(train.shape[0])\n",
    "    \n",
    "    # defining an empty list to collect the roc_auc score of each fold\n",
    "    roc_score   = []\n",
    "    \n",
    "    # defining te criteria of split\n",
    "    folds       = StratifiedKFold(n_splits=NSPLITS,random_state=42,shuffle=True)\n",
    "    \n",
    "    # Cross Validation\n",
    "    for fold_, (train_index,test_index) in enumerate(folds.split(train[features],train[TARGET_COL])):\n",
    "        print(f'\\n------------- Fold {fold_ + 1} -------------')\n",
    "        \n",
    "        # dividing the data into folds\n",
    "        x_train, x_val = train[features].iloc[train_index], train[features].iloc[test_index]\n",
    "        y_train, y_val = train[TARGET_COL].iloc[train_index], train[TARGET_COL].iloc[test_index]\n",
    "        \n",
    "        # fitting the model\n",
    "        _ = clf.fit(x_train, \n",
    "                    y_train, \n",
    "                    eval_set = [(x_val, y_val)], \n",
    "                    **fit_params)\n",
    "        \n",
    "        \n",
    "        # predicting on validation data and appending the score to roc_score list\n",
    "        preds = clf.predict_proba(x_val)[:, 1]\n",
    "        score = roc_auc_score(y_val,preds)\n",
    "        print (\"Validation ROC AUC Score:\",score)\n",
    "        print (\"------------------------------------------------\")\n",
    "        roc_score.append(score)\n",
    "        \n",
    "        # oofs predictions\n",
    "        oofs[test_index] = preds\n",
    "    \n",
    "        # Predicting on the test set\n",
    "        predictions += clf.predict_proba(test)[:, 1] / folds.n_splits\n",
    "    \n",
    "    \n",
    "    # plotting Feature Importance\n",
    "    plt.figure(figsize=(6,8));\n",
    "    ser = pd.Series(clf.feature_importances_,test.columns).sort_values();\n",
    "    ser.plot(kind='barh');\n",
    "    plt.title(\"Feature Importance Plot\");\n",
    "    plt.xlabel(\"Feature\");\n",
    "    plt.ylabel(\"Importance\");\n",
    "    \n",
    "    # Mean roc_score of 5 folds. \n",
    "    print (\"------------------------------------------\")\n",
    "    print (f\"Mean ROC AUC Score of {NSPLITS} Folds:\",np.mean(np.array(roc_score)))\n",
    "    \n",
    "    return oofs, predictions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
